%% If you have issues compiling this file, you can try using knitr instead of sweave
%% to do so, go to tools-->global options-->sweave
%% and change the "weave rnw files using:" to knitr

%% If the compiler gets stuck, restart R or erase the pdf so that it can be compiled again

\documentclass{beamer}
%\usetheme{Madrid}
%\usetheme{Antibes}
%\usetheme{CambridgeUS}
% \usetheme{Goettingen}
\usecolortheme{beaver}
\usepackage{textpos}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage[T1]{fontenc}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{url}
%\usepackage[hyphenbreaks]{breakurl}
\usepackage{graphicx, verbatim}
\usepackage{hanging}
\usepackage{amsmath}
\usepackage{amscd}
\usepackage{lipsum}
\usepackage{todonotes}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{setspace}
\usepackage{float}
\usepackage{listings}
\usepackage{hyperref}

\usepackage{Sweave}
                                      
\setbeamercovered{dynamic}
%\setbeamercovered{invisible}
\setbeamertemplate{footnote}{\hangpara{2em}{1}\makebox[2em][l]{\insertfootnotemark}\footnotesize\insertfootnotetext\par}

\useinnertheme{rectangles}

\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor    = red   %Colour of citations
}

% \usepackage{lipsum}
% \AtBeginSection[]
%   {
%      \begin{frame}<beamer>
%      \frametitle{Outline}
%      \tableofcontents[currentsection, currentsubsection]
%      \end{frame}
%   }

\date{}
\begin{document}
% uncomment if using sweave to compile
% \SweaveOpts{concordance=TRUE} 


\begin{frame}
\title{Introduction to R Workshop \\ (Session 3)}
\author{Dr Carlos Moreno-Garcia}{\includegraphics[scale=.4]{figure/rgu.jpg}}
\institute{School of Computing Science and Digital Media \\  Robert Gordon University }
\maketitle
\end{frame}


\addtobeamertemplate{frametitle}{}{%
\begin{textblock*}{30mm}(.70\textwidth,-1cm)
\includegraphics[scale=.3]{figure/rgu.jpg}
\end{textblock*}}

\section{Outline}
\frame{\tableofcontents[pausesections]}\frametitle{Outline}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Machine Learning Overview}
\AtBeginSection[]
  {
     \begin{frame}<beamer>
     \frametitle{Outline}
     \tableofcontents[currentsection]
     \end{frame}
  }
  
\subsection{Overview}

\begin{frame}[fragile] \frametitle{Overview}

\begin{center}
\begin{figure}[hb]
  \centering
  \includegraphics[width=3 in]{figure/overview.jpg}
\end{figure}
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Terminology}
\begin{frame}[fragile] \frametitle{Machine Learning - Terminology}

\begin{itemize}
\item Feature Space
\item Class or Target
\item Supervised Machine Learning
\item Unsupervised Machine Learning
\item Training Set
\item Testing Set
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Formal Definitions}
\begin{frame}[fragile] \frametitle{Machine Learning - Formal Definitions}

A dataset $A$ with $m$ instances $\textbf{x}_{1}, \textbf{x}_{2}, ..., \textbf{x}_{m}$, where each instance $\textbf{x}_{i}$ is defined by an $n$ features as $\textbf {x}_{i}=(x_{i1}, x_{i2}, ..., x_{in})$. \\

In a typical supervised machine learning scenario, these instances are often labelled or categorised. 

\begin{equation} 
\label{eq1}
A = \begin{bmatrix}
 x_{11}  & x_{12}  & ..., & x_{1n}\\ 
...  & x_{22}  & ..., & ...\\ 
...  & ...  & ...& ...\\ 
 
x_{m1}  & ...  & ..., & x_{mn}\\ 
\end{bmatrix},
Y=
\begin{bmatrix}
y_{1}\\ 
..\\ 
..\\ 
y_{m}

\end{bmatrix}
\end{equation}

Learn a function $\textit{h(x)}$ that maps an instance $\textbf{x}_{i} \in A$ to a class $\textbf{y}_{j}\in Y$.

Notice that if $Y$ is a set of discrete values then we call this a \textbf{classification} problem, otherwise it is called a \textbf{regression} problem.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Classification}
\subsection{Regression}
\begin{frame}[fragile] \frametitle{Supervised Machine Learning}
\textbf{Regressions}

A regression is possibly the simplest form of machine learning. It is based on establishing a function based on a set of points, where $x$ is the feature and $y$ is the target.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Regression - Example 1}

\begin{columns}[c]
\column{.35\textwidth}

\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
No & X & Y \\ 
  \hline
1 &   1 & 3.00 \\ 
  2 &   2 & 8.00 \\ 
  3 &   3 & 15.00 \\ 
  4 &   4 & 24.00 \\ 
  ...&   ... & ... \\
  18 &  18 & 360.00 \\ 
  19 &  19 & 399.00 \\ 
  20 &  20 & 440.00 \\ 
   \hline
\end{tabular}
\end{table}

\column{.65\textwidth}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=4,fig.height=4,out.width='.99\\linewidth',echo=FALSE>>=

x <- seq(1:10)
y <- 2*x +(x*x)
plot(x,y,type='b',pch=21, bg='black',frame=FALSE)
# df <- data.frame(X=x, Y=y)
# library(xtable)
# print(xtable(df))

@

\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Regression - Example 1}

Linear Regression
$h(x_0)=\theta_0+\theta_1x$

\begin{columns}[c]
\column{.45\textwidth}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=55,width.cutoff=60),fig.width=4,fig.height=4,out.width='.85\\linewidth',echo=TRUE,eval=FALSE>>=

x <- seq(1:10)
y <- 2*x +(x*x)
df <- data.frame(X=x, Y=y)
plot(df$X,df$Y,type='b',
     pch=21,frame=FALSE)
fit <- lm( Y ~ X,data=df)
abline(fit,col='red')

@

\column{.55\textwidth}

<<echo=FALSE,eval=TRUE>>=

x <- seq(1:10)
y <- 2*x +(x*x)
df <- data.frame(X=x, Y=y)

@

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=4,fig.height=4,out.width='.9\\linewidth',echo=FALSE,eval=TRUE>>=

plot(df$X,df$Y,type='b',
     xlab="X",ylab="Y",frame=FALSE)
fit <- lm( Y ~ X,data=df)
abline(fit,col='red')

@

\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Regression - Example 1}

Regressions with ggplot

\begin{columns}[c]
\column{.45\textwidth}

<<tidy=FALSE,warning=FALSE,message=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=4,fig.height=4,out.width='.8\\linewidth',echo=TRUE,eval=TRUE>>=

require(ggplot2)
x <- seq(1:10)
y <- 2*x +(x*x)
df <- data.frame(X=x, 
                 Y=y)

@

\column{.55\textwidth}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=4,fig.height=4,out.width='.7\\linewidth',echo=TRUE,eval=TRUE>>=

ggplot(df, aes(x = X, y = Y)) + 
  geom_point() +
  stat_smooth(method = "lm", 
              col = "red") +
    theme_bw()

@

\end{columns}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Regression - Example 2}

\item Consider the \textit{cats} dataset.

\begin{columns}[c]
\column{.90\textwidth}

<<echo=TRUE,eval=FALSE>>=

library(MASS)
data(cats)
require(ggplot2)
p <-  ggplot(cats, aes(x = Bwt, y = Hwt)) 
p <- p + labs(x="Body Weight",y="Heart Weight")
p <- p + geom_point(aes(col=Sex)) 
p <- p + stat_smooth(method = "lm")
p <- p + facet_wrap(~Sex)
p <- p + theme_bw()
p <- p + theme(legend.title = element_blank())
p <- p + theme(legend.position='none')
p

@

\end{columns}
\ \\
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Regression - Example 2}

\begin{center}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=6,fig.height=4,out.width='.8\\linewidth',echo=FALSE,eval=TRUE>>=

library(MASS)
require(ggplot2)
data(cats)
p <-  ggplot(cats, aes(x = Bwt, y = Hwt)) 
p <- p + labs(x="Body Weight",y="Heart Weight")
p <- p + geom_point(aes(col=Sex)) 
p <- p + stat_smooth(method = "lm")
p <- p + facet_wrap(~Sex)
p <- p + theme_bw()
p <- p + theme(legend.title = element_blank())
p <- p + theme(legend.position='none')
p

@

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Clustering}
\begin{frame}[fragile] \frametitle{Unsupervised Machine Learning}
\textbf{K-means}

K-means clustering is a method of classifying/grouping items into \textit{k} groups, where \textit{k} is the number of clusters). The grouping is done by minimizing the sum of squared distances (i.e. Euclidean distance). This method (in its most basic form) does NOT take the target into consideration.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Clustering - Example}

\item Using the Petal.width and Petal.length features of the \textit{iris} dataset, We will use the \textit{ggplot2} package to apply k-means to cluster and visualise the data.

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Clustering - Example}
\begin{center}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=6,fig.height=4,out.width='.99\\linewidth',echo=TRUE,eval=FALSE>>=
df=iris
m=as.matrix(cbind(df$Sepal.Length, 
        df$Petal.Width),ncol=2)
cl=(kmeans(m,3))
df$cluster=factor(cl$cluster)
centers=as.data.frame(cl$centers)
p <- ggplot(data=df, aes(x=Petal.Length, 
        y=Petal.Width,color=cluster),size=.2,alpha=.4) + 
     geom_point() + 
     geom_point(data=centers, 
                aes(x=V1,y=V2, color='Center')) +
     geom_point(data=centers, 
                aes(x=V1,y=V2, color='Center'), 
                size=50, alpha=.2) +theme_bw()
p <- p + theme(legend.title = element_blank())
p <- p + theme(legend.position='none')
p
@
\end{center}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile] \frametitle{Clustering - Example}

\begin{center}

<<tidy=FALSE,idy.opts=list(blank=FALSE,width=50,width.cutoff=60),fig.width=6,fig.height=4,out.width='.88\\linewidth',echo=FALSE,eval=TRUE>>=

df=iris
m=as.matrix(cbind(df$Petal.Length, 
        df$Petal.Width),ncol=2)
cl=(kmeans(m,3))
df$cluster=factor(cl$cluster)
centers=as.data.frame(cl$centers)
p <- ggplot(data=df, aes(x=Petal.Length, 
            y=Petal.Width,color=cluster),size=.2,alpha=.4) + 
     geom_point() + 
     geom_point(data=centers, 
                aes(x=V1,y=V2, color='Center')) +
     geom_point(data=centers, 
                aes(x=V1,y=V2, color='Center'), 
                size=50, alpha=.2) +theme_bw()
p <- p + theme(legend.title = element_blank())
p <- p + theme(legend.position='none')
p

@

\end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile] \frametitle{Final advise}

\begin{itemize}
\item Split your data into test, train (and validation).
\item Select the right feature set.
\item There is a handful of machine learning techniques!
\end{itemize}
\end{frame}

\end{document}
